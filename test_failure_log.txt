============================= test session starts =============================
platform win32 -- Python 3.14.0, pytest-9.0.2, pluggy-1.6.0 -- C:\Users\User\OneDrive\Documents\GitHub\arifOS\.venv\Scripts\python.exe
cachedir: .pytest_cache
rootdir: C:\Users\User\OneDrive\Documents\GitHub\arifOS
configfile: pyproject.toml
plugins: anyio-4.12.0, cov-7.0.0
collecting ... collected 7 items

tests/test_orthogonal_bundles.py::test_agi_think_hard_lane PASSED        [ 14%]
tests/test_orthogonal_bundles.py::test_agi_think_phatic PASSED           [ 28%]
tests/test_orthogonal_bundles.py::test_asi_act_safe PASSED               [ 42%]
tests/test_orthogonal_bundles.py::test_asi_act_aggressive_veto PASSED    [ 57%]
tests/test_orthogonal_bundles.py::test_asi_act_credential_veto PASSED    [ 71%]
tests/test_orthogonal_bundles.py::test_apex_audit_seal FAILED            [ 85%]
tests/test_orthogonal_bundles.py::test_apex_audit_asi_fail FAILED        [100%]

================================== FAILURES ===================================
____________________________ test_apex_audit_seal _____________________________

    def test_apex_audit_seal():
        """Test APEX seals when all conditions met."""
        agi_out = {"verdict": "PASS", "side_data": {"thought_process": "Plan A"}}
        asi_out = {"verdict": "PASS", "side_data": {"peace_score": 1.1}}
        # Strong evidence (3 sources)
        evidence = {"sources": ["A", "B", "C"]}
    
        req = ApexAuditRequest(
            agi_thought=agi_out,
            asi_veto=asi_out,
            evidence_pack=evidence
        )
>       res = asyncio.run(apex_audit(req))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\test_orthogonal_bundles.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
..\..\..\..\AppData\Local\Programs\Python\Python314\Lib\asyncio\runners.py:204: in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
..\..\..\..\AppData\Local\Programs\Python\Python314\Lib\asyncio\runners.py:127: in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
..\..\..\..\AppData\Local\Programs\Python\Python314\Lib\asyncio\base_events.py:719: in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
arifos_core\mcp\tools\bundles\apex_audit.py:96: in apex_audit
    verdict_obj = judge.judge_output(
arifos_core\system\apex_prime.py:146: in judge_output
    metrics = self._calculate_trinity_metrics(all_floors)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
arifos_core\system\apex_prime.py:241: in _calculate_trinity_metrics
    verdict = evaluate_genius_law(m)
              ^^^^^^^^^^^^^^^^^^^^^^
arifos_core\enforcement\genius_metrics.py:720: in evaluate_genius_law
    delta = compute_delta_score(m)
            ^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

m = Metrics(truth=1.0, delta_s=1.0, peace_squared=1.1, kappa_r=1.0, omega_0=0.04, amanah=True, tri_witness=0.95, rasa=True..._load=None, dignity_rma_ok=True, vault_consistent=True, behavior_drift_ok=True, ontology_ok=True, sleeper_scan_ok=True)

    def compute_delta_score(m: Metrics) -> float:
        """
        Compute \u0394 (Delta/Clarity) score from Metrics.
    
        \u0394 maps to Akal (A) \u2014 cognitive clarity, pattern recognition.
        Derived from: truth, delta_s (clarity floor).
    
        Formula:
            \u0394 = (truth_ratio + clarity_ratio) / 2
    
        Where:
            truth_ratio = truth / TRUTH_THRESHOLD (clamped to [0, 1])
            clarity_ratio = 1.0 if delta_s >= 0 else (1 + delta_s) clamped to [0, 1]
    
        Args:
            m: Metrics instance
    
        Returns:
            \u0394 score in [0, 1] range
        """
        # Truth ratio (clamped)
        truth_ratio = min(m.truth / TRUTH_THRESHOLD, 1.0) if TRUTH_THRESHOLD > 0 else 1.0
    
        # Clarity ratio (delta_s >= 0 is good)
>       if m.delta_s >= DELTA_S_THRESHOLD:
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: '>=' not supported between instances of 'float' and 'str'

arifos_core\enforcement\genius_metrics.py:258: TypeError
__________________________ test_apex_audit_asi_fail ___________________________

    def test_apex_audit_asi_fail():
        """Test APEX voids if ASI failed."""
        agi_out = {"verdict": "PASS"}
        asi_out = {"verdict": "VOID", "reason": "Harmful"}
        evidence = {"sources": ["A", "B", "C"]}
    
        req = ApexAuditRequest(
            agi_thought=agi_out,
            asi_veto=asi_out,
            evidence_pack=evidence
        )
>       res = asyncio.run(apex_audit(req))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests\test_orthogonal_bundles.py:97: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
..\..\..\..\AppData\Local\Programs\Python\Python314\Lib\asyncio\runners.py:204: in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
..\..\..\..\AppData\Local\Programs\Python\Python314\Lib\asyncio\runners.py:127: in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
..\..\..\..\AppData\Local\Programs\Python\Python314\Lib\asyncio\base_events.py:719: in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
arifos_core\mcp\tools\bundles\apex_audit.py:96: in apex_audit
    verdict_obj = judge.judge_output(
arifos_core\system\apex_prime.py:146: in judge_output
    metrics = self._calculate_trinity_metrics(all_floors)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
arifos_core\system\apex_prime.py:241: in _calculate_trinity_metrics
    verdict = evaluate_genius_law(m)
              ^^^^^^^^^^^^^^^^^^^^^^
arifos_core\enforcement\genius_metrics.py:720: in evaluate_genius_law
    delta = compute_delta_score(m)
            ^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

m = Metrics(truth=1.0, delta_s=1.0, peace_squared=1.0, kappa_r=1.0, omega_0=0.04, amanah=True, tri_witness=0.95, rasa=Fals..._load=None, dignity_rma_ok=True, vault_consistent=True, behavior_drift_ok=True, ontology_ok=True, sleeper_scan_ok=True)

    def compute_delta_score(m: Metrics) -> float:
        """
        Compute \u0394 (Delta/Clarity) score from Metrics.
    
        \u0394 maps to Akal (A) \u2014 cognitive clarity, pattern recognition.
        Derived from: truth, delta_s (clarity floor).
    
        Formula:
            \u0394 = (truth_ratio + clarity_ratio) / 2
    
        Where:
            truth_ratio = truth / TRUTH_THRESHOLD (clamped to [0, 1])
            clarity_ratio = 1.0 if delta_s >= 0 else (1 + delta_s) clamped to [0, 1]
    
        Args:
            m: Metrics instance
    
        Returns:
            \u0394 score in [0, 1] range
        """
        # Truth ratio (clamped)
        truth_ratio = min(m.truth / TRUTH_THRESHOLD, 1.0) if TRUTH_THRESHOLD > 0 else 1.0
    
        # Clarity ratio (delta_s >= 0 is good)
>       if m.delta_s >= DELTA_S_THRESHOLD:
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: '>=' not supported between instances of 'float' and 'str'

arifos_core\enforcement\genius_metrics.py:258: TypeError
============================== warnings summary ===============================
arifos_core\apex\governance\fag.py:83
  C:\Users\User\OneDrive\Documents\GitHub\arifOS\arifos_core\apex\governance\fag.py:83: DeprecationWarning: arifos_core.apex.governance.ledger is deprecated. Use arifos_core.state.ledger instead. This shim will be removed in v47.2 (72 hours after v47.1).
    from .ledger import log_cooling_entry

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED tests/test_orthogonal_bundles.py::test_apex_audit_seal - TypeError: '>...
FAILED tests/test_orthogonal_bundles.py::test_apex_audit_asi_fail - TypeError...
=================== 2 failed, 5 passed, 1 warning in 2.44s ====================
