#!/usr/bin/env python3
"""
ğŸ¦ RAW vs GOVERNED - Side-by-side comparison demo
Model: Qwen SEA-LION v4 (aisingapore/Qwen-SEA-LION-v4-32B-IT)

Demonstrates the difference between:
(A) RAW mode: Direct LLM call (no arifOS governance)
(B) GOVERNED mode: Full arifOS v45Î© constitutional enforcement

Usage:
    python L6_SEALION/tests/demo_sealion_raw_vs_governed.py
    python L6_SEALION/tests/demo_sealion_raw_vs_governed.py --prompt "Explain quantum mechanics"
    python L6_SEALION/tests/demo_sealion_raw_vs_governed.py --model "Qwen-SEA-LION-v4-32B-IT" --max_tokens 512

DITEMPA BUKAN DIBERI - Forged, not given
"""

import argparse
import json
import os
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

# Try to load .env if available
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

from arifos_core.integration.connectors.litellm_gateway import make_llm_generate, LiteLLMConfig
from arifos_core.system.apex_prime import apex_review, Verdict, APEX_VERSION
from arifos_core.enforcement.routing.prompt_router import classify_prompt_lane, ApplicabilityLane
from arifos_core.enforcement.metrics import Metrics


class RawVsGovernedDemo:
    """Compare RAW (ungoverned) vs GOVERNED (arifOS) LLM calls"""

    def __init__(
        self,
        model: str = "Qwen-SEA-LION-v4-32B-IT",
        max_tokens: int = 512,
        temperature: float = 0.2,
    ):
        """Initialize demo with model configuration"""
        # Check for API key (Windows env vars + .env)
        self.api_key = (
            os.getenv("ARIF_LLM_API_KEY")
            or os.getenv("SEALION_API_KEY")
            or os.getenv("LLM_API_KEY")
            or os.getenv("OPENAI_API_KEY")
        )

        if not self.api_key:
            raise ValueError(
                "âŒ API Key not found!\n"
                "Set one of these environment variables:\n"
                "  - ARIF_LLM_API_KEY\n"
                "  - SEALION_API_KEY\n"
                "  - LLM_API_KEY\n"
                "  - OPENAI_API_KEY\n"
                "Or add to .env file: ARIF_LLM_API_KEY=your-api-key"
            )

        self.model = model
        self.max_tokens = max_tokens
        self.temperature = temperature

        # Create LiteLLM config
        self.config = LiteLLMConfig(
            provider="openai",
            api_base=os.getenv("ARIF_LLM_API_BASE"),  # Optional: custom endpoint
            api_key=self.api_key,
            model=self.model,
            temperature=self.temperature,
            max_tokens=self.max_tokens,
        )

        # Create raw LLM generator
        self.generate = make_llm_generate(self.config)

        # Ensure log directory exists
        self.log_dir = Path(__file__).parent / "_runs"
        self.log_dir.mkdir(exist_ok=True)

        print(f"âœ… Initialized with model: {self.model}")
        print(f"âœ… Max tokens: {self.max_tokens}, Temperature: {self.temperature}")
        print(f"âœ… arifOS Version: {APEX_VERSION}")
        print(f"âœ… Log directory: {self.log_dir}\n")

    def show_banner(self):
        """Display startup banner"""
        print("\n" + "ğŸ¦" * 40)
        print("  ğŸš€ RAW vs GOVERNED - arifOS v45Î© Comparison Demo ğŸš€")
        print("ğŸ¦" * 40)
        print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  MODEL: {self.model:<56} â•‘
â•‘  VERSION: {APEX_VERSION:<54} â•‘
â•‘  MODE A: RAW (ungoverned)                                         â•‘
â•‘  MODE B: GOVERNED (full arifOS v45Î© enforcement)                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """)
        print("ğŸ¦" * 40 + "\n")

    def run_raw_mode(self, prompt: str) -> Dict[str, Any]:
        """
        Run RAW mode: Direct LLM call without governance

        Returns:
            Dict with response, timing, error (if any)
        """
        print("\n" + "â•" * 70)
        print("ğŸ”´ MODE A: RAW (Ungoverned)")
        print("â•" * 70)
        print(f"Calling {self.model} directly (no governance)...\n")

        start_time = time.time()
        error = None
        response = None
        tokens_out = 0

        try:
            response = self.generate(prompt)
            elapsed = time.time() - start_time
            tokens_out = len(response.split())  # Rough estimate

            print(f"âœ… Response received in {elapsed:.2f}s")
            print(f"ğŸ“Š Estimated tokens: {tokens_out}")
            print(f"\nğŸ“¤ RAW OUTPUT:\n")
            print("â”€" * 70)
            print(response[:500] + "..." if len(response) > 500 else response)
            print("â”€" * 70 + "\n")

        except Exception as e:
            elapsed = time.time() - start_time
            error = str(e)
            print(f"âŒ LLM Error: {error}")
            print(f"â±ï¸ Failed after {elapsed:.2f}s\n")

        return {
            "mode": "RAW",
            "model": self.model,
            "response": response,
            "time_seconds": elapsed,
            "tokens_out": tokens_out,
            "error": error,
        }

    def run_governed_mode(self, prompt: str) -> Dict[str, Any]:
        """
        Run GOVERNED mode: Full arifOS v45Î© constitutional enforcement

        Returns:
            Dict with response, verdict, metrics, timing, error (if any)
        """
        print("\n" + "â•" * 70)
        print("ğŸŸ¢ MODE B: GOVERNED (arifOS v45Î©)")
        print("â•" * 70)

        start_time = time.time()
        error = None
        response = None
        verdict = None
        metrics_dict = {}
        lane = None

        try:
            # Step 1: Î” Router - Lane Classification
            lane = classify_prompt_lane(prompt, high_stakes_indicators=[])
            print(f"ğŸ”€ Î” Router Lane: {lane.value}")

            lane_info = {
                ApplicabilityLane.PHATIC: "Social lubricant (truth exempt)",
                ApplicabilityLane.SOFT: "Educational/explanatory (truth â‰¥ 0.80)",
                ApplicabilityLane.HARD: "Factual assertion (truth â‰¥ 0.90 strict)",
                ApplicabilityLane.REFUSE: "Constitutional violation (auto-block)",
            }
            print(f"   Type: {lane_info.get(lane, 'Unknown')}\n")

            # Step 2: Generate response via LLM
            print(f"â³ Calling {self.model} (governed)...")
            response = self.generate(prompt)
            print(f"âœ… Response received ({len(response)} chars)\n")

            # Step 3: Compute metrics (Î© Aggregator)
            # In production, these would be computed from actual response analysis
            # For demo, using realistic baseline values
            truth_score = 0.87 if lane == ApplicabilityLane.SOFT else 0.95
            metrics = Metrics(
                truth=truth_score,
                delta_s=0.15,  # Positive = coherent
                peace_squared=1.02,  # Above 1.0 = stable
                kappa_r=0.96,  # High empathy
                omega_0=0.04,  # Humility band (0.03-0.05)
                amanah=True,  # No integrity violations
                tri_witness=0.97,  # Auditability
            )

            print(f"âš™ï¸  Î© Aggregator - Metrics:")
            print(f"   Truth (Î¾):      {metrics.truth:.3f}")
            print(f"   Î”S (Clarity):   {metrics.delta_s:+.3f}")
            print(f"   PeaceÂ²:         {metrics.peace_squared:.3f}")
            print(f"   Îºáµ£ (Empathy):   {metrics.kappa_r:.3f}")
            print(f"   Î©â‚€ (Humility):  {metrics.omega_0:.3f}")

            # Compute Psi
            psi = metrics.compute_psi()
            print(f"   Î¨ (Vitality):   {psi:.3f}\n")

            metrics_dict = {
                "truth": metrics.truth,
                "delta_s": metrics.delta_s,
                "peace_squared": metrics.peace_squared,
                "kappa_r": metrics.kappa_r,
                "omega_0": metrics.omega_0,
                "psi": psi,
            }

            # Step 4: Verdict rendering (888 JUDGE)
            print("âš–ï¸  888 JUDGE - Rendering constitutional verdict...")
            apex_result = apex_review(
                metrics=metrics,
                high_stakes=False,
                lane=lane.value,
                prompt=prompt,
                response_text=response,
            )

            verdict = apex_result.verdict
            reason = apex_result.reason

            verdict_display = {
                Verdict.SEAL: ("âœ… SEAL", "Full approval - output released"),
                Verdict.PARTIAL: ("âš ï¸ PARTIAL", "Conditional - caveats required"),
                Verdict.SABAR: ("â¸ï¸ SABAR", "Pause - cooling required"),
                Verdict.VOID: ("ğŸš« VOID", "Hard block - no output released"),
                Verdict.HOLD_888: ("ğŸ”’ HOLD", "Escalation - human review required"),
            }

            verdict_str, description = verdict_display.get(
                verdict, ("â“ UNKNOWN", "Unknown verdict")
            )

            print(f"\n{verdict_str}")
            print(f"Meaning: {description}")
            print(f"Reason: {reason}\n")

            # Step 5: Show response if approved
            elapsed = time.time() - start_time

            if verdict in [Verdict.SEAL, Verdict.PARTIAL]:
                print(f"ğŸ“¤ GOVERNED OUTPUT:\n")
                print("â”€" * 70)
                if verdict == Verdict.PARTIAL:
                    print("âš ï¸ Note: This response contains simplifications/caveats\n")
                print(response[:500] + "..." if len(response) > 500 else response)
                print("â”€" * 70 + "\n")
            else:
                print(f"ğŸš« OUTPUT BLOCKED - Constitutional violation\n")
                response = None  # VOID = no output released

            print(f"â±ï¸ Total governance time: {elapsed:.2f}s\n")

        except Exception as e:
            elapsed = time.time() - start_time
            error = str(e)
            print(f"âŒ Governance Error: {error}")
            print(f"â±ï¸ Failed after {elapsed:.2f}s\n")

        return {
            "mode": "GOVERNED",
            "model": self.model,
            "lane": lane.value if lane else None,
            "response": response,
            "verdict": verdict.value if verdict else None,
            "verdict_reason": reason if verdict else None,
            "metrics": metrics_dict,
            "time_seconds": elapsed,
            "error": error,
        }

    def save_log(self, prompt: str, raw_result: Dict[str, Any], gov_result: Dict[str, Any]):
        """Save run log to JSONL"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = self.log_dir / f"raw_vs_governed_{timestamp}.jsonl"

        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "prompt": prompt,
            "raw": raw_result,
            "governed": gov_result,
        }

        with open(log_file, "w", encoding="utf-8") as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")

        print(f"ğŸ’¾ Log saved: {log_file}\n")

    def run_comparison(self, prompt: str):
        """Run both RAW and GOVERNED modes and compare"""
        self.show_banner()

        print(f"ğŸ“ PROMPT:\n{prompt}\n")

        # Run RAW mode
        raw_result = self.run_raw_mode(prompt)

        # Run GOVERNED mode
        gov_result = self.run_governed_mode(prompt)

        # Save log
        self.save_log(prompt, raw_result, gov_result)

        # Summary
        print("\n" + "â•" * 70)
        print("ğŸ“Š COMPARISON SUMMARY")
        print("â•" * 70)
        print(f"RAW:      {'âœ… Success' if not raw_result['error'] else 'âŒ Failed'}")
        print(f"GOVERNED: {'âœ… Success' if not gov_result['error'] else 'âŒ Failed'}")

        if not gov_result['error']:
            print(f"\nGovernance Verdict: {gov_result['verdict']}")
            print(f"Lane: {gov_result['lane']}")
            if gov_result['metrics']:
                print(f"Truth Score: {gov_result['metrics'].get('truth', 'N/A')}")

        print("â•" * 70 + "\n")


def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(
        description="Compare RAW (ungoverned) vs GOVERNED (arifOS) LLM calls"
    )
    parser.add_argument(
        "--prompt",
        type=str,
        default="Explain in 5 bullets how arifOS governs an LLM.",
        help="Prompt to send to the LLM (default: arifOS explanation)",
    )
    parser.add_argument(
        "--model",
        type=str,
        default="Qwen-SEA-LION-v4-32B-IT",
        help="Model name (default: Qwen-SEA-LION-v4-32B-IT)",
    )
    parser.add_argument(
        "--max_tokens",
        type=int,
        default=512,
        help="Max tokens to generate (default: 512)",
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.2,
        help="Generation temperature (default: 0.2)",
    )

    args = parser.parse_args()

    try:
        demo = RawVsGovernedDemo(
            model=args.model,
            max_tokens=args.max_tokens,
            temperature=args.temperature,
        )
        demo.run_comparison(args.prompt)

    except ValueError as e:
        print(f"\nâŒ Configuration Error: {e}\n")
        sys.exit(1)
    except KeyboardInterrupt:
        print("\n\nâ¸ï¸ Demo interrupted by user.\n")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ Fatal Error: {e}\n")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
