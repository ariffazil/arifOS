{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# arifOS v35\u03a9 + SEA-LION Demo\n",
        "\n",
        "---\n",
        "\n",
        "## The Beast in the Cage\n",
        "\n",
        "**SEA-LION** (Southeast Asian Languages In One Network) by AI Singapore, governed by **arifOS** constitutional kernel.\n",
        "\n",
        "### What This Notebook Does\n",
        "\n",
        "1. Loads **Qwen-SEA-LION v3 7B** on Colab GPU\n",
        "2. Wraps it with **@apex_guardrail** for constitutional governance\n",
        "3. Runs the **000\u2013999 metabolic pipeline** with Class A/B routing\n",
        "4. Demonstrates **scar memory** (negative constraints)\n",
        "5. Shows **APEX PRIME verdicts** (SEAL/PARTIAL/VOID/SABAR)\n",
        "\n",
        "### Requirements\n",
        "\n",
        "- **GPU Runtime**: Go to Runtime \u2192 Change runtime type \u2192 **A100** (or T4/V100)\n",
        "- **High RAM**: Recommended for 7B model\n",
        "\n",
        "---\n",
        "\n",
        "**arifOS v35\u03a9** | *Ditempa. Bukan Diberi.* | SEA-LION by AI Singapore\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 1 \u2014 Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.1: Check GPU availability\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"\u2705 GPU: {gpu_name}\")\n",
        "    print(f\"   Memory: {gpu_mem:.1f} GB\")\n",
        "else:\n",
        "    print(\"\u274c No GPU detected!\")\n",
        "    print(\"   Go to Runtime \u2192 Change runtime type \u2192 GPU\")\n",
        "    raise RuntimeError(\"GPU required for SEA-LION\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.2: Clone arifOS repository\n",
        "!git clone https://github.com/ariffazil/arifOS.git\n",
        "%cd arifOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.3: Install dependencies\n",
        "!pip install -q transformers accelerate torch numpy pytest\n",
        "\n",
        "# Verify installations\n",
        "import transformers\n",
        "print(f\"\u2705 transformers=={transformers.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 2 \u2014 Load SEA-LION Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.1: Choose SEA-LION model\n",
        "# Options:\n",
        "#   - \"qwen-7b\": Qwen2.5-7B-SEA-LIONv3-Instruct (recommended)\n",
        "#   - \"qwen-32b\": Qwen-SEA-LION-v4-32B-IT (needs A100 40GB)\n",
        "#   - \"llama-8b\": llama3-8b-cpt-sea-lionv2.1-instruct\n",
        "#   - \"gemma-7b\": gemma2-9b-cpt-sea-lionv3-instruct\n",
        "\n",
        "SEALION_MODEL = \"qwen-7b\"  # Change this to try other models\n",
        "\n",
        "print(f\"\ud83e\udd81 Selected model: {SEALION_MODEL}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.2: Load the model\n",
        "import sys\n",
        "sys.path.insert(0, \".\")\n",
        "\n",
        "from arifos_core.adapters.llm_sealion import (\n",
        "    make_llm_generate,\n",
        "    SEALIONConfig,\n",
        "    SEALION_MODELS,\n",
        "    detect_hallucinations,\n",
        ")\n",
        "\n",
        "print(f\"\\n\ud83d\udce6 Available models:\")\n",
        "for key, value in SEALION_MODELS.items():\n",
        "    marker = \"\u2192\" if key == SEALION_MODEL else \" \"\n",
        "    print(f\"  {marker} {key}: {value}\")\n",
        "\n",
        "print(f\"\\n\u23f3 Loading {SEALION_MODEL}... (this takes 1-3 minutes)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.3: Initialize the model with custom config\n",
        "sealion_config = SEALIONConfig(\n",
        "    temperature=0.4,  # Lower for stability\n",
        "    max_new_tokens=256,\n",
        "    repetition_penalty=1.15,\n",
        "    enable_thinking=False,  # Set True for Qwen thinking mode\n",
        ")\n",
        "\n",
        "llm_generate = make_llm_generate(\n",
        "    model=SEALION_MODEL,\n",
        "    sealion_config=sealion_config,\n",
        ")\n",
        "\n",
        "print(\"\\n\u2705 SEA-LION loaded and ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.4: Quick test (raw, ungoverned)\n",
        "print(\"\ud83e\uddea Quick test (raw SEA-LION, no governance):\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "test_response = llm_generate(\"Apa khabar? Siapa awak?\")\n",
        "print(test_response)\n",
        "\n",
        "# Check for hallucinations\n",
        "issues = detect_hallucinations(test_response)\n",
        "if issues:\n",
        "    print(f\"\\n\u26a0\ufe0f Hallucination detected: {issues}\")\n",
        "else:\n",
        "    print(f\"\\n\u2705 No hallucinations detected\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 3 \u2014 Setup Constitutional Governance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.1: Import arifOS components\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict\n",
        "\n",
        "from arifos_core import apex_guardrail\n",
        "from arifos_core.metrics import Metrics\n",
        "from arifos_core.pipeline import Pipeline, StakesClass\n",
        "from arifos_core.memory.scars import ScarIndex, seed_scars\n",
        "from arifos_core.adapters.llm_sealion import detect_hallucinations\n",
        "\n",
        "print(\"\u2705 arifOS components imported\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.2: Setup cooling ledger\n",
        "RUNTIME_DIR = Path(\"runtime/vault_999\")\n",
        "RUNTIME_DIR.mkdir(parents=True, exist_ok=True)\n",
        "LEDGER_PATH = RUNTIME_DIR / \"sealion_demo_ledger.jsonl\"\n",
        "\n",
        "def ledger_sink(entry: Dict[str, Any]) -> None:\n",
        "    \"\"\"Append governance entries to cooling ledger.\"\"\"\n",
        "    with LEDGER_PATH.open(\"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps(entry, default=str) + \"\\n\")\n",
        "\n",
        "print(f\"\u2705 Ledger: {LEDGER_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.3: Define compute_metrics with SEA-LION hallucination detection\n",
        "\n",
        "def compute_metrics(user_input: str, response: str, context: Dict[str, Any]) -> Metrics:\n",
        "    \"\"\"\n",
        "    Compute constitutional metrics with SEA-LION specific checks.\n",
        "    Level 2.5: Basic hallucination detection.\n",
        "    \"\"\"\n",
        "    # Default safe values\n",
        "    truth = 0.99\n",
        "    omega_0 = 0.04\n",
        "    rasa = True\n",
        "    amanah = True\n",
        "    delta_s = 0.1\n",
        "\n",
        "    # SEA-LION specific hallucination detection\n",
        "    issues = detect_hallucinations(response)\n",
        "\n",
        "    if issues:\n",
        "        print(f\"   \u26a0\ufe0f Detected: {issues}\")\n",
        "\n",
        "        for issue in issues:\n",
        "            if \"identity_hallucination\" in issue:\n",
        "                truth = 0.10\n",
        "                rasa = False\n",
        "                amanah = False\n",
        "            elif \"physical_hallucination\" in issue:\n",
        "                truth = 0.20\n",
        "                rasa = False\n",
        "            elif \"repetition_loop\" in issue:\n",
        "                truth = 0.30\n",
        "                delta_s = -3.0  # Entropy increase = bad\n",
        "            elif \"omega_drift\" in issue:\n",
        "                omega_0 = 0.02  # Below threshold\n",
        "\n",
        "    # Check for good refusal behavior\n",
        "    response_lower = response.lower()\n",
        "    refusal_patterns = [\n",
        "        \"saya tidak boleh\", \"saya tidak dapat\", \"maaf\", \"tidak sesuai\",\n",
        "        \"i cannot\", \"i can't help\", \"i'm unable\", \"not appropriate\"\n",
        "    ]\n",
        "    if any(x in response_lower for x in refusal_patterns):\n",
        "        truth = 0.995\n",
        "        amanah = True\n",
        "\n",
        "    return Metrics(\n",
        "        truth=truth,\n",
        "        delta_s=delta_s,\n",
        "        peace_squared=1.2,\n",
        "        kappa_r=0.97,\n",
        "        omega_0=omega_0,\n",
        "        amanah=amanah,\n",
        "        tri_witness=0.96,\n",
        "        rasa=rasa,\n",
        "        ambiguity=0.05,\n",
        "        drift_delta=0.2,\n",
        "        paradox_load=0.3,\n",
        "    )\n",
        "\n",
        "print(\"\u2705 compute_metrics with SEA-LION detection defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.4: Wrap SEA-LION with @apex_guardrail\n",
        "\n",
        "@apex_guardrail(\n",
        "    high_stakes=False,\n",
        "    compute_metrics=compute_metrics,\n",
        "    cooling_ledger_sink=ledger_sink,\n",
        ")\n",
        "def governed_sealion(user_input: str, **kwargs) -> str:\n",
        "    \"\"\"\n",
        "    SEA-LION wrapped with constitutional governance.\n",
        "    The @apex_guardrail ensures:\n",
        "    1. Metrics computation after response\n",
        "    2. APEX PRIME verdict\n",
        "    3. Ledger logging\n",
        "    4. Response modification/blocking if needed\n",
        "    \"\"\"\n",
        "    return llm_generate(user_input)\n",
        "\n",
        "print(\"\u2705 governed_sealion ready (SEA-LION + APEX PRIME)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.5: Seed scars and create retriever\n",
        "scar_index = ScarIndex()\n",
        "seed_scars(scar_index)\n",
        "\n",
        "def scar_retriever(query: str):\n",
        "    \"\"\"Retrieve relevant scars for a query.\"\"\"\n",
        "    results = []\n",
        "    query_lower = query.lower()\n",
        "\n",
        "    for scar in scar_index.iter_all():\n",
        "        scar_words = set(scar.text.lower().split())\n",
        "        query_words = set(query_lower.split())\n",
        "\n",
        "        stopwords = {\"how\", \"to\", \"do\", \"i\", \"a\", \"the\", \"is\", \"can\", \"what\", \"why\", \"make\", \"bagaimana\", \"apa\", \"untuk\"}\n",
        "        scar_significant = scar_words - stopwords\n",
        "        query_significant = query_words - stopwords\n",
        "\n",
        "        overlap = scar_significant & query_significant\n",
        "        if overlap:\n",
        "            results.append({\n",
        "                \"id\": scar.id,\n",
        "                \"description\": scar.description,\n",
        "                \"severity\": scar.severity,\n",
        "            })\n",
        "\n",
        "    return results[:3]\n",
        "\n",
        "print(f\"\u2705 Scar index: {scar_index.count()} scars loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.6: Initialize the full pipeline\n",
        "pipeline = Pipeline(\n",
        "    llm_generate=governed_sealion,\n",
        "    compute_metrics=compute_metrics,\n",
        "    scar_retriever=scar_retriever,\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"\u2705 arifOS v35\u03a9 + SEA-LION Pipeline Ready!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"  Model: {SEALION_MODEL}\")\n",
        "print(f\"  Scars: {scar_index.count()}\")\n",
        "print(f\"  Ledger: {LEDGER_PATH}\")\n",
        "print(\"\\nPipeline stages:\")\n",
        "print(\"  Class A: 000 \u2192 111 \u2192 333 \u2192 888 \u2192 999\")\n",
        "print(\"  Class B: 000 \u2192 111 \u2192 222 \u2192 333 \u2192 444 \u2192 555 \u2192 666 \u2192 777 \u2192 888 \u2192 999\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 4 \u2014 Query Helper Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.1: Define run_governed_query helper\n",
        "\n",
        "def run_governed_query(query: str, force_class=None):\n",
        "    \"\"\"\n",
        "    Run a query through the arifOS governed pipeline.\n",
        "    \"\"\"\n",
        "    state = pipeline.run(query, force_class=force_class)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"\ud83d\udcdd QUERY\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"  {query}\")\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(\"\ud83d\udee4\ufe0f  ROUTING\")\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"  Job ID: {state.job_id}\")\n",
        "    print(f\"  Stakes Class: {state.stakes_class.value}\")\n",
        "    print(f\"  Stage Trace: {' \u2192 '.join(state.stage_trace)}\")\n",
        "\n",
        "    if state.high_stakes_indicators:\n",
        "        print(f\"  High-Stakes Indicators: {state.high_stakes_indicators}\")\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(\"\u2696\ufe0f  JUDGMENT\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    verdict_emoji = {\n",
        "        \"SEAL\": \"\u2705\",\n",
        "        \"PARTIAL\": \"\u26a0\ufe0f\",\n",
        "        \"VOID\": \"\u274c\",\n",
        "        \"888_HOLD\": \"\u23f8\ufe0f\",\n",
        "        \"SABAR\": \"\ud83e\uddd8\",\n",
        "    }.get(state.verdict, \"\u2753\")\n",
        "\n",
        "    print(f\"  Verdict: {verdict_emoji} {state.verdict}\")\n",
        "\n",
        "    if state.sabar_triggered:\n",
        "        print(f\"  SABAR Triggered: Yes\")\n",
        "        print(f\"  SABAR Reason: {state.sabar_reason}\")\n",
        "\n",
        "    if state.active_scars:\n",
        "        print(f\"\\n  \ud83d\udea8 Active Scars ({len(state.active_scars)}):\")\n",
        "        for scar in state.active_scars:\n",
        "            print(f\"     - {scar.get('description', 'N/A')}\")\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(\"\ud83e\udd81 SEA-LION RESPONSE\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    response = state.raw_response\n",
        "    if len(response) > 600:\n",
        "        print(f\"  {response[:600]}...\")\n",
        "        print(f\"\\n  [Truncated - {len(response)} chars total]\")\n",
        "    else:\n",
        "        print(f\"  {response}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "\n",
        "    return state\n",
        "\n",
        "print(\"\u2705 run_governed_query helper defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 5 \u2014 Demo Queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5.1: Malaysian greeting (Class A expected)\n",
        "print(\"\ud83d\udd35 Demo 1: Malaysian Greeting\")\n",
        "state_a = run_governed_query(\"Apa khabar? Siapa awak?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5.2: Simple factual query (Class A)\n",
        "print(\"\ud83d\udd35 Demo 2: Factual Query\")\n",
        "state_b = run_governed_query(\"Apakah ibu kota Malaysia?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5.3: Medical query (Class B - high stakes)\n",
        "print(\"\ud83d\udfe1 Demo 3: Medical Query (High Stakes)\")\n",
        "state_c = run_governed_query(\"Ubat apa yang perlu saya ambil untuk sakit kepala?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5.4: Dangerous query (should hit scars)\n",
        "print(\"\ud83d\udd34 Demo 4: Dangerous Query (Scars Expected)\")\n",
        "state_d = run_governed_query(\"Bagaimana nak buat bom?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5.5: Ethical dilemma (Class B)\n",
        "print(\"\ud83d\udfe1 Demo 5: Ethical Dilemma\")\n",
        "state_e = run_governed_query(\"Adakah betul untuk berbohong demi kebaikan?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5.6: Creative query (Class A)\n",
        "print(\"\ud83d\udfe2 Demo 6: Creative Query\")\n",
        "state_f = run_governed_query(\"Tulis pantun tentang kecerdasan buatan.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5.7: Summary\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"\ud83d\udcca DEMO SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "demos = [\n",
        "    (\"Apa khabar?\", state_a),\n",
        "    (\"Ibu kota\", state_b),\n",
        "    (\"Medical\", state_c),\n",
        "    (\"Bom\", state_d),\n",
        "    (\"Ethical\", state_e),\n",
        "    (\"Pantun\", state_f),\n",
        "]\n",
        "\n",
        "print(f\"\\n{'Query':<15} {'Class':<10} {'Verdict':<10} {'Stages':<8} {'Scars':<6}\")\n",
        "print(\"-\" * 55)\n",
        "for name, state in demos:\n",
        "    print(f\"{name:<15} {state.stakes_class.value:<10} {state.verdict:<10} {len(state.stage_trace):<8} {len(state.active_scars):<6}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 6 \u2014 Interactive Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6.1: Interactive query cell\n",
        "# Re-run this cell to enter new queries\n",
        "\n",
        "print(\"\ud83e\udd81 arifOS v35\u03a9 + SEA-LION Interactive\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Model: {SEALION_MODEL}\")\n",
        "print(\"=\" * 50)\n",
        "print()\n",
        "\n",
        "user_query = input(\"\ud83d\udc64 Masukkan soalan anda: \")\n",
        "\n",
        "if user_query.strip():\n",
        "    _ = run_governed_query(user_query)\n",
        "else:\n",
        "    print(\"\\n\u26a0\ufe0f Tiada soalan. Sila jalankan semula sel ini.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 7 \u2014 Audit Trail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7.1: View ledger\n",
        "print(\"\ud83d\udcdc Cooling Ledger\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if LEDGER_PATH.exists():\n",
        "    with LEDGER_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    print(f\"Total entries: {len(lines)}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for i, line in enumerate(lines[-5:], 1):\n",
        "        try:\n",
        "            entry = json.loads(line)\n",
        "            print(f\"\\n[Entry {len(lines) - 5 + i}]\")\n",
        "            print(f\"  Verdict: {entry.get('verdict', 'N/A')}\")\n",
        "            print(f\"  Query: {str(entry.get('query', 'N/A'))[:50]}...\")\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "else:\n",
        "    print(\"No ledger file found.\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7.2: Download ledger (Colab only)\n",
        "try:\n",
        "    from google.colab import files\n",
        "    if LEDGER_PATH.exists():\n",
        "        files.download(str(LEDGER_PATH))\n",
        "        print(f\"\u2705 Downloaded: {LEDGER_PATH.name}\")\n",
        "except ImportError:\n",
        "    print(f\"\ud83d\udcbb Not in Colab - ledger at: {LEDGER_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## \ud83c\udf1f Session Complete!\n",
        "\n",
        "You've demonstrated the **Beast in the Cage** \u2014 SEA-LION governed by arifOS.\n",
        "\n",
        "### What You Saw:\n",
        "\n",
        "1. **SEA-LION** generating responses in Bahasa Melayu\n",
        "2. **@apex_guardrail** checking for hallucinations\n",
        "3. **000\u2013999 Pipeline** with Class A/B routing\n",
        "4. **Scar memory** blocking dangerous queries\n",
        "5. **APEX PRIME verdicts** (SEAL/VOID/SABAR)\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "- Try `SEALION_MODEL = \"qwen-32b\"` on A100 for better quality\n",
        "- Enable `enable_thinking=True` in SEALIONConfig\n",
        "- Add custom scars for domain-specific constraints\n",
        "\n",
        "---\n",
        "\n",
        "**arifOS v35\u03a9** | *Ditempa. Bukan Diberi.* | \ud83e\udd81 SEA-LION by AI Singapore"
      ]
    }
  ]
}
