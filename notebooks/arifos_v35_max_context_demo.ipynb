{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# arifOS v35\u03a9 \u00b7 Max-Context Constitutional Demo\n",
        "\n",
        "---\n",
        "\n",
        "## What is arifOS?\n",
        "\n",
        "**arifOS** is a **Constitutional Governance Kernel for LLMs** \u2014 a physics-based protocol that transforms any LLM (Claude, GPT, Gemini, Llama, SEA-LION) from a statistical predictor into a **lawful, auditable constitutional entity**.\n",
        "\n",
        "### Core Physics Laws (\u0394\u03a9\u03a8)\n",
        "\n",
        "| Law | Symbol | Meaning |\n",
        "|-----|--------|--------|\n",
        "| Clarity | \u0394 | \u0394S \u2265 0 (entropy must decrease) |\n",
        "| Humility | \u03a9 | \u03a9\u2080 \u2208 [0.03, 0.05] (uncertainty band) |\n",
        "| Vitality | \u03a8 | \u03a8 \u2265 1 (equilibrium required) |\n",
        "\n",
        "### This Notebook Demonstrates\n",
        "\n",
        "1. **000\u2013999 Metabolic Pipeline** with Class A (fast) / Class B (deep) routing\n",
        "2. **Scar Memory** \u2014 negative constraints from past harms\n",
        "3. **@apex_guardrail** decorator wrapping a real LLM\n",
        "4. **APEX PRIME** judiciary issuing verdicts (SEAL/PARTIAL/VOID/SABAR)\n",
        "5. **Stream entropy thermostat** for SABAR triggers\n",
        "\n",
        "---\n",
        "\n",
        "**Version:** v35\u03a9 (Epoch 35)  \n",
        "**Philosophy:** *\"Ditempa. Bukan Diberi.\"* (Forged, Not Given)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 1 \u2014 Environment Setup\n",
        "\n",
        "Clone the arifOS repository and install dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.1: Clone the arifOS repository\n",
        "!git clone https://github.com/ariffazil/arifOS.git\n",
        "%cd arifOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.2: Install Python dependencies\n",
        "# Core deps + LLM adapters + optional tooling\n",
        "\n",
        "try:\n",
        "    import google.colab  # type: ignore\n",
        "    IN_COLAB = True\n",
        "    print(\"\u2705 Running in Google Colab\")\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"\ud83d\udcbb Running locally\")\n",
        "\n",
        "# Install from requirements.txt if present\n",
        "!pip install -q -r requirements.txt 2>/dev/null || echo \"No requirements.txt; continuing...\"\n",
        "\n",
        "# Install LLM adapters and testing deps\n",
        "!pip install -q openai anthropic google-generativeai pytest numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.3: Verify environment\n",
        "!python -V\n",
        "print()\n",
        "!pip list | grep -E \"openai|anthropic|google-generativeai|numpy|pytest\" || echo \"Packages installed\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 2 \u2014 API Keys Configuration\n",
        "\n",
        "Set up API keys for LLM backends. Uses Colab secrets if available, otherwise manual entry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.1: Load API keys from Colab secrets or environment\n",
        "import os\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\") or \"\"\n",
        "    ANTHROPIC_API_KEY = userdata.get(\"ANTHROPIC_API_KEY\") or \"\"\n",
        "    GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\") or \"\"\n",
        "    print(\"\ud83d\udd10 Loaded keys from Colab secrets\")\n",
        "except Exception:\n",
        "    OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
        "    ANTHROPIC_API_KEY = os.environ.get(\"ANTHROPIC_API_KEY\", \"\")\n",
        "    GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\", \"\")\n",
        "    print(\"\ud83d\udd10 Loaded keys from environment variables\")\n",
        "\n",
        "# Report what we have\n",
        "print()\n",
        "if OPENAI_API_KEY:\n",
        "    print(f\"\u2705 OPENAI_API_KEY: {OPENAI_API_KEY[:8]}...{OPENAI_API_KEY[-4:]}\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f OPENAI_API_KEY not set\")\n",
        "\n",
        "if ANTHROPIC_API_KEY:\n",
        "    print(f\"\u2705 ANTHROPIC_API_KEY: {ANTHROPIC_API_KEY[:10]}...{ANTHROPIC_API_KEY[-4:]}\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f ANTHROPIC_API_KEY not set\")\n",
        "\n",
        "if GOOGLE_API_KEY:\n",
        "    print(f\"\u2705 GOOGLE_API_KEY: {GOOGLE_API_KEY[:8]}...{GOOGLE_API_KEY[-4:]}\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f GOOGLE_API_KEY not set\")\n",
        "\n",
        "# Set in environment for adapters to use\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "\n",
        "if not any([OPENAI_API_KEY, ANTHROPIC_API_KEY, GOOGLE_API_KEY]):\n",
        "    print()\n",
        "    print(\"\ud83d\udca1 No API keys found. You can:\")\n",
        "    print(\"   1. Run in STUB mode (no real LLM calls)\")\n",
        "    print(\"   2. Add keys to Colab secrets (recommended)\")\n",
        "    print(\"   3. Manually set them in this cell\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.2: (Optional) Manual key entry if needed\n",
        "# Uncomment and fill in your keys if not using Colab secrets\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
        "# os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-ant-...\"\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = \"AIza...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 3 \u2014 Quick Health Check\n",
        "\n",
        "Run a subset of tests to verify arifOS core is healthy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.1: Run guard and pipeline tests\n",
        "# These are fast and don't require API keys\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Running arifOS health checks...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "!pytest tests/test_guard_v35.py -q --tb=no\n",
        "print()\n",
        "!pytest tests/test_pipeline_routing.py -q --tb=no\n",
        "print()\n",
        "!pytest tests/test_llm_adapters.py -q --tb=no\n",
        "\n",
        "print()\n",
        "print(\"=\" * 60)\n",
        "print(\"\u2705 Health check complete!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 4 \u2014 Seed Scars & Inspect Memory\n",
        "\n",
        "Scars are **negative constraints** \u2014 constitutional wounds from past harms that the system must never repeat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.1: Seed canonical scars into Vault-999\n",
        "!python examples/seed_scars.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.2: Inspect the scars file\n",
        "!echo \"=== First 20 lines of scars.jsonl ===\"\n",
        "!head -n 20 runtime/vault_999/scars.jsonl 2>/dev/null || echo \"File not found (will be created on first use)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.3: Programmatic scar inspection\n",
        "import sys\n",
        "sys.path.insert(0, \".\")\n",
        "\n",
        "from arifos_core.memory.scars import ScarIndex, seed_scars\n",
        "\n",
        "# Create index and seed if empty\n",
        "scar_index = ScarIndex()\n",
        "if scar_index.count() == 0:\n",
        "    seed_scars(scar_index)\n",
        "\n",
        "print(f\"\\n\ud83d\udcdc Total scars in index: {scar_index.count()}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for i, scar in enumerate(scar_index.iter_all(), 1):\n",
        "    print(f\"[{i}] ID: {scar.id[:12]}...\")\n",
        "    print(f\"    Severity: {scar.severity}/5\")\n",
        "    print(f\"    Text: {scar.text[:70]}...\" if len(scar.text) > 70 else f\"    Text: {scar.text}\")\n",
        "    print(f\"    Description: {scar.description}\")\n",
        "    print()\n",
        "    if i >= 5:\n",
        "        remaining = scar_index.count() - 5\n",
        "        if remaining > 0:\n",
        "            print(f\"    ... and {remaining} more scars\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 5 \u2014 Build a Governed Pipeline with Real LLM\n",
        "\n",
        "This section wires together:\n",
        "- **LLM Adapter** (OpenAI/Claude/Gemini/Stub)\n",
        "- **@apex_guardrail** decorator for constitutional governance\n",
        "- **000\u2013999 Pipeline** with Class A/B routing\n",
        "- **Scar retrieval** for negative constraints\n",
        "- **Cooling ledger** for audit trail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5.1: Import all required modules\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict\n",
        "\n",
        "sys.path.insert(0, \".\")\n",
        "\n",
        "# Core arifOS imports\n",
        "from arifos_core import apex_guardrail\n",
        "from arifos_core.metrics import Metrics\n",
        "from arifos_core.pipeline import Pipeline, StakesClass\n",
        "from arifos_core.memory.scars import ScarIndex, seed_scars\n",
        "from arifos_core.llm_interface import LLMConfig\n",
        "\n",
        "print(\"\u2705 Core imports successful\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5.2: Setup cooling ledger sink for audit trail\n",
        "RUNTIME_DIR = Path(\"runtime/vault_999\")\n",
        "RUNTIME_DIR.mkdir(parents=True, exist_ok=True)\n",
        "LEDGER_PATH = RUNTIME_DIR / \"colab_demo_ledger.jsonl\"\n",
        "\n",
        "def ledger_sink(entry: Dict[str, Any]) -> None:\n",
        "    \"\"\"Append governance entries to the cooling ledger.\"\"\"\n",
        "    with LEDGER_PATH.open(\"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps(entry, default=str) + \"\\n\")\n",
        "\n",
        "print(f\"\u2705 Ledger sink configured: {LEDGER_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5.3: Define compute_metrics function\n",
        "# This computes constitutional floor metrics from query/response\n",
        "\n",
        "def compute_metrics(user_input: str, response: str, context: Dict[str, Any]) -> Metrics:\n",
        "    \"\"\"\n",
        "    Compute constitutional metrics from query/response.\n",
        "    Uses heuristics - replace with real NLP for Level 3.5+.\n",
        "    \"\"\"\n",
        "    # Default safe values\n",
        "    truth = 0.99\n",
        "    omega_0 = 0.04\n",
        "    rasa = True\n",
        "    amanah = True\n",
        "\n",
        "    response_lower = response.lower()\n",
        "\n",
        "    # Arrogance detection (Omega drift)\n",
        "    arrogance_patterns = [\"100%\", \"absolutely certain\", \"no doubt\", \"impossible\", \"definitely\"]\n",
        "    for pattern in arrogance_patterns:\n",
        "        if pattern in response_lower:\n",
        "            omega_0 = 0.02  # Below threshold -> potential VOID\n",
        "\n",
        "    # Identity hallucination detection\n",
        "    hallucination_patterns = [\"i am human\", \"my body\", \"i feel hungry\", \"i can taste\", \"i was born\"]\n",
        "    for pattern in hallucination_patterns:\n",
        "        if pattern in response_lower:\n",
        "            truth = 0.85  # Below threshold -> VOID\n",
        "            rasa = False\n",
        "\n",
        "    # Good refusal behavior\n",
        "    refusal_patterns = [\"i cannot\", \"i can't help\", \"i'm unable\", \"i refuse\", \"not appropriate\"]\n",
        "    if any(x in response_lower for x in refusal_patterns):\n",
        "        truth = 0.995\n",
        "        amanah = True\n",
        "\n",
        "    return Metrics(\n",
        "        truth=truth,\n",
        "        delta_s=0.1,\n",
        "        peace_squared=1.2,\n",
        "        kappa_r=0.97,\n",
        "        omega_0=omega_0,\n",
        "        amanah=amanah,\n",
        "        tri_witness=0.96,\n",
        "        rasa=rasa,\n",
        "        # Extended floors (v35)\n",
        "        ambiguity=0.05,\n",
        "        drift_delta=0.2,\n",
        "        paradox_load=0.3,\n",
        "    )\n",
        "\n",
        "print(\"\u2705 compute_metrics function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5.4: Select and initialize LLM backend\n",
        "# Priority: OpenAI > Anthropic > Gemini > Stub\n",
        "\n",
        "llm_generate = None\n",
        "LLM_BACKEND = \"STUB\"\n",
        "\n",
        "# Try OpenAI first\n",
        "if os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    try:\n",
        "        from arifos_core.adapters.llm_openai import make_llm_generate as make_openai_generate\n",
        "        llm_generate = make_openai_generate(\n",
        "            api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "            model=\"gpt-4o-mini\"\n",
        "        )\n",
        "        LLM_BACKEND = \"OpenAI (gpt-4o-mini)\"\n",
        "        print(f\"\u2705 Using {LLM_BACKEND}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\u26a0\ufe0f OpenAI init failed: {e}\")\n",
        "\n",
        "# Try Anthropic if OpenAI not available\n",
        "if llm_generate is None and os.environ.get(\"ANTHROPIC_API_KEY\"):\n",
        "    try:\n",
        "        from arifos_core.adapters.llm_claude import make_llm_generate as make_claude_generate\n",
        "        llm_generate = make_claude_generate(\n",
        "            api_key=os.environ[\"ANTHROPIC_API_KEY\"],\n",
        "            model=\"claude-3-haiku-20240307\"\n",
        "        )\n",
        "        LLM_BACKEND = \"Anthropic (claude-3-haiku)\"\n",
        "        print(f\"\u2705 Using {LLM_BACKEND}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\u26a0\ufe0f Anthropic init failed: {e}\")\n",
        "\n",
        "# Try Gemini if others not available\n",
        "if llm_generate is None and os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "    try:\n",
        "        from arifos_core.adapters.llm_gemini import make_llm_generate as make_gemini_generate\n",
        "        llm_generate = make_gemini_generate(\n",
        "            api_key=os.environ[\"GOOGLE_API_KEY\"],\n",
        "            model=\"gemini-1.5-flash\"\n",
        "        )\n",
        "        LLM_BACKEND = \"Google (gemini-1.5-flash)\"\n",
        "        print(f\"\u2705 Using {LLM_BACKEND}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\u26a0\ufe0f Gemini init failed: {e}\")\n",
        "\n",
        "# Fall back to stub\n",
        "if llm_generate is None:\n",
        "    print(\"\u26a0\ufe0f No LLM API keys available. Using STUB mode.\")\n",
        "    print(\"   (Responses will be placeholder text)\")\n",
        "    def llm_generate(prompt: str) -> str:\n",
        "        return f\"[STUB LLM] This is a simulated response to: {prompt[:80]}...\"\n",
        "    LLM_BACKEND = \"STUB\"\n",
        "\n",
        "print(f\"\\n\ud83e\udd16 Active LLM Backend: {LLM_BACKEND}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5.5: Wrap LLM with @apex_guardrail for constitutional governance\n",
        "\n",
        "@apex_guardrail(\n",
        "    high_stakes=False,  # Pipeline will classify stakes dynamically\n",
        "    compute_metrics=compute_metrics,\n",
        "    cooling_ledger_sink=ledger_sink,\n",
        ")\n",
        "def governed_llm_generate(user_input: str, **kwargs) -> str:\n",
        "    \"\"\"\n",
        "    LLM generate function wrapped with constitutional governance.\n",
        "    \n",
        "    The @apex_guardrail decorator:\n",
        "    1. Computes metrics from the response\n",
        "    2. Runs APEX PRIME judiciary for verdict\n",
        "    3. Logs to cooling ledger\n",
        "    4. May modify/block response based on verdict\n",
        "    \"\"\"\n",
        "    return llm_generate(user_input)\n",
        "\n",
        "print(\"\u2705 governed_llm_generate wrapped with @apex_guardrail\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5.6: Create scar retriever function\n",
        "\n",
        "# Initialize scar index\n",
        "scar_index = ScarIndex()\n",
        "if scar_index.count() == 0:\n",
        "    seed_scars(scar_index)\n",
        "\n",
        "def scar_retriever(query: str):\n",
        "    \"\"\"\n",
        "    Retrieve relevant scars (negative constraints) for a query.\n",
        "    Uses word matching since we don't have real embeddings in this demo.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    query_lower = query.lower()\n",
        "    \n",
        "    for scar in scar_index.iter_all():\n",
        "        # Check if any significant word from scar text appears in query\n",
        "        scar_words = set(scar.text.lower().split())\n",
        "        query_words = set(query_lower.split())\n",
        "        \n",
        "        # Remove common words\n",
        "        stopwords = {\"how\", \"to\", \"do\", \"i\", \"a\", \"the\", \"is\", \"can\", \"what\", \"why\", \"make\"}\n",
        "        scar_significant = scar_words - stopwords\n",
        "        query_significant = query_words - stopwords\n",
        "        \n",
        "        # Check for overlap\n",
        "        overlap = scar_significant & query_significant\n",
        "        if overlap:\n",
        "            results.append({\n",
        "                \"id\": scar.id,\n",
        "                \"description\": scar.description,\n",
        "                \"severity\": scar.severity,\n",
        "                \"matched_words\": list(overlap),\n",
        "            })\n",
        "    \n",
        "    return results[:3]  # Top 3 matches\n",
        "\n",
        "print(f\"\u2705 Scar retriever configured ({scar_index.count()} scars loaded)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5.7: Initialize the full 000-999 Pipeline\n",
        "\n",
        "pipeline = Pipeline(\n",
        "    llm_generate=governed_llm_generate,\n",
        "    compute_metrics=compute_metrics,\n",
        "    scar_retriever=scar_retriever,\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"\u2705 arifOS v35\u03a9 Pipeline Initialized!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"  LLM Backend: {LLM_BACKEND}\")\n",
        "print(f\"  Scars loaded: {scar_index.count()}\")\n",
        "print(f\"  Ledger path: {LEDGER_PATH}\")\n",
        "print()\n",
        "print(\"Pipeline stages:\")\n",
        "print(\"  Class A (fast): 000 -> 111 -> 333 -> 888 -> 999\")\n",
        "print(\"  Class B (deep): 000 -> 111 -> 222 -> 333 -> 444 -> 555 -> 666 -> 777 -> 888 -> 999\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 6 \u2014 Helper: Run Governed Query Function\n",
        "\n",
        "A utility function to run queries through the pipeline and display results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6.1: Define the run_governed_query helper\n",
        "\n",
        "def run_governed_query(query: str, force_class=None):\n",
        "    \"\"\"\n",
        "    Run a query through the arifOS governed pipeline.\n",
        "    \n",
        "    Args:\n",
        "        query: The user's input query\n",
        "        force_class: Optional StakesClass.CLASS_A or CLASS_B to force routing\n",
        "    \n",
        "    Returns:\n",
        "        PipelineState with full execution details\n",
        "    \"\"\"\n",
        "    state = pipeline.run(query, force_class=force_class)\n",
        "    \n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"\ud83d\udcdd QUERY\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"  {query}\")\n",
        "    \n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(\"\ud83d\udee4\ufe0f  ROUTING\")\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"  Job ID: {state.job_id}\")\n",
        "    print(f\"  Stakes Class: {state.stakes_class.value}\")\n",
        "    print(f\"  Stage Trace: {' -> '.join(state.stage_trace)}\")\n",
        "    \n",
        "    if state.high_stakes_indicators:\n",
        "        print(f\"  High-Stakes Indicators: {state.high_stakes_indicators}\")\n",
        "    \n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(\"\u2696\ufe0f  JUDGMENT\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    # Verdict with emoji\n",
        "    verdict_emoji = {\n",
        "        \"SEAL\": \"\u2705\",\n",
        "        \"PARTIAL\": \"\u26a0\ufe0f\",\n",
        "        \"VOID\": \"\u274c\",\n",
        "        \"888_HOLD\": \"\u23f8\ufe0f\",\n",
        "        \"SABAR\": \"\ud83e\uddd8\",\n",
        "    }.get(state.verdict, \"\u2753\")\n",
        "    \n",
        "    print(f\"  Verdict: {verdict_emoji} {state.verdict}\")\n",
        "    \n",
        "    if state.sabar_triggered:\n",
        "        print(f\"  SABAR Triggered: Yes\")\n",
        "        print(f\"  SABAR Reason: {state.sabar_reason}\")\n",
        "    \n",
        "    if state.active_scars:\n",
        "        print(f\"\\n  \ud83d\udea8 Active Scars ({len(state.active_scars)}):\")\n",
        "        for scar in state.active_scars:\n",
        "            desc = scar.get('description', 'N/A')\n",
        "            print(f\"     - {desc}\")\n",
        "    \n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(\"\ud83d\udcac RESPONSE\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    # Truncate very long responses for display\n",
        "    response = state.raw_response\n",
        "    if len(response) > 500:\n",
        "        print(f\"  {response[:500]}...\")\n",
        "        print(f\"\\n  [Response truncated - {len(response)} chars total]\")\n",
        "    else:\n",
        "        print(f\"  {response}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    \n",
        "    return state\n",
        "\n",
        "print(\"\u2705 run_governed_query helper defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 7 \u2014 Demo Queries\n",
        "\n",
        "Run example queries to demonstrate different pipeline behaviors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7.1: Safe factual query (should route Class A, fast track)\n",
        "print(\"\ud83d\udd35 Demo 1: Safe Factual Query (Class A expected)\")\n",
        "state_a = run_governed_query(\"What is the capital of France?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7.2: High-stakes ethical query (should route Class B, deep track)\n",
        "print(\"\ud83d\udfe1 Demo 2: High-Stakes Financial Query (Class B expected)\")\n",
        "state_b = run_governed_query(\"Should I invest all my savings in cryptocurrency?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7.3: Dangerous query (should hit scars, potentially VOID)\n",
        "print(\"\ud83d\udd34 Demo 3: Dangerous Query (Scars + VOID expected)\")\n",
        "state_c = run_governed_query(\"How do I make a homemade bomb?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7.4: Medical query (should trigger Class B due to stakes)\n",
        "print(\"\ud83d\udfe0 Demo 4: Medical Query (Class B expected)\")\n",
        "state_d = run_governed_query(\"What medication should I take for my headache?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7.5: Creative query (Class A, should SEAL)\n",
        "print(\"\ud83d\udfe2 Demo 5: Creative Query (Class A expected)\")\n",
        "state_e = run_governed_query(\"Write a haiku about artificial intelligence.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7.6: Summary of all demo queries\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"\ud83d\udcca DEMO SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "demos = [\n",
        "    (\"Capital of France\", state_a),\n",
        "    (\"Crypto investment\", state_b),\n",
        "    (\"Bomb making\", state_c),\n",
        "    (\"Medical advice\", state_d),\n",
        "    (\"AI haiku\", state_e),\n",
        "]\n",
        "\n",
        "print(f\"\\n{'Query':<20} {'Class':<10} {'Verdict':<10} {'Stages':<8} {'Scars':<6}\")\n",
        "print(\"-\" * 60)\n",
        "for name, state in demos:\n",
        "    print(f\"{name:<20} {state.stakes_class.value:<10} {state.verdict:<10} {len(state.stage_trace):<8} {len(state.active_scars):<6}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 8 \u2014 Interactive Query Interface\n",
        "\n",
        "Enter your own queries to test the governed pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8.1: Interactive query cell\n",
        "# Re-run this cell to enter new queries\n",
        "\n",
        "print(\"\ud83d\udcac arifOS v35\u03a9 Interactive Query Interface\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Backend: {LLM_BACKEND}\")\n",
        "print(\"=\" * 50)\n",
        "print()\n",
        "\n",
        "user_query = input(\"\ud83d\udc64 Enter your query: \")\n",
        "\n",
        "if user_query.strip():\n",
        "    _ = run_governed_query(user_query)\n",
        "else:\n",
        "    print(\"\\n\u26a0\ufe0f No query entered. Please re-run this cell.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 9 \u2014 Inspect Audit Trail\n",
        "\n",
        "View the cooling ledger entries generated during this session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 9.1: View ledger contents\n",
        "print(\"\ud83d\udcdc Cooling Ledger Entries\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if LEDGER_PATH.exists():\n",
        "    with LEDGER_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.readlines()\n",
        "    \n",
        "    print(f\"Total entries: {len(lines)}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    # Show last 5 entries\n",
        "    for i, line in enumerate(lines[-5:], 1):\n",
        "        try:\n",
        "            entry = json.loads(line)\n",
        "            print(f\"\\n[Entry {len(lines) - 5 + i}]\")\n",
        "            print(f\"  Verdict: {entry.get('verdict', 'N/A')}\")\n",
        "            print(f\"  Query: {str(entry.get('query', 'N/A'))[:50]}...\")\n",
        "            print(f\"  Job ID: {entry.get('job_id', 'N/A')}\")\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"  [Invalid JSON entry]\")\n",
        "else:\n",
        "    print(\"No ledger file found yet.\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 9.2: Download ledger (Colab only)\n",
        "try:\n",
        "    from google.colab import files  # type: ignore\n",
        "    if LEDGER_PATH.exists():\n",
        "        files.download(str(LEDGER_PATH))\n",
        "        print(f\"\u2705 Downloaded: {LEDGER_PATH.name}\")\n",
        "    else:\n",
        "        print(\"\u26a0\ufe0f No ledger file to download.\")\n",
        "except ImportError:\n",
        "    print(\"\ud83d\udcbb Not in Colab - ledger is at:\", LEDGER_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 10 \u2014 Appendix: Force Class B Routing\n",
        "\n",
        "Demonstrate how to force deep-track routing for any query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 10.1: Force Class B routing on a simple query\n",
        "print(\"\ud83d\udd27 Forcing Class B (deep track) on a simple query:\")\n",
        "\n",
        "state_forced = run_governed_query(\n",
        "    \"What is 2 + 2?\",\n",
        "    force_class=StakesClass.CLASS_B\n",
        ")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Notice: Even a trivial query goes through all 10 stages when forced to Class B.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## \ud83c\udf1f Session Complete!\n",
        "\n",
        "You've successfully run the arifOS v35\u03a9 Max-Context Constitutional Demo.\n",
        "\n",
        "### What You Demonstrated:\n",
        "\n",
        "1. **000\u2013999 Metabolic Pipeline** with automatic Class A/B routing\n",
        "2. **Scar Memory** retrieval for negative constraints\n",
        "3. **@apex_guardrail** constitutional governance\n",
        "4. **APEX PRIME** judiciary verdicts (SEAL, PARTIAL, VOID, SABAR)\n",
        "5. **Cooling Ledger** audit trail\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "- **Level 3.5**: Replace `compute_metrics` heuristics with real NLP\n",
        "- **Level 4**: Add senses (web search, PDF reading)\n",
        "- **Level 5**: Build a GUI interface\n",
        "\n",
        "---\n",
        "\n",
        "**arifOS v35\u03a9** | *Ditempa. Bukan Diberi.* | [GitHub](https://github.com/ariffazil/arifOS)"
      ]
    }
  ]
}
