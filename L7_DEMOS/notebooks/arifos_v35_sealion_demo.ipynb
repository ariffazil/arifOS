{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# arifOS v35 + SEA-LION Demo\n",
        "\n",
        "---\n",
        "\n",
        "## The Beast in the Cage\n",
        "\n",
        "**SEA-LION** (Southeast Asian Languages In One Network) by AI Singapore, governed by **arifOS** constitutional kernel.\n",
        "\n",
        "### What This Notebook Does\n",
        "\n",
        "1. Loads **Llama-SEA-LION v3 8B** on Colab GPU\n",
        "2. Wraps it with **@apex_guardrail** for constitutional governance\n",
        "3. Runs the **000-999 metabolic pipeline** with Class A/B routing\n",
        "4. Demonstrates **scar memory** (negative constraints)\n",
        "5. Shows **APEX PRIME verdicts** (SEAL/PARTIAL/VOID/SABAR)\n",
        "\n",
        "### Requirements\n",
        "\n",
        "- **GPU Runtime**: Runtime > Change runtime type > **T4** or **A100**\n",
        "- **High RAM**: Recommended for 8B model\n",
        "\n",
        "---\n",
        "\n",
        "**arifOS v35** | *Ditempa. Bukan Diberi.* | SEA-LION by AI Singapore\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 1 - Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.1: Check GPU availability\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"GPU: {gpu_name}\")\n",
        "    print(f\"   Memory: {gpu_mem:.1f} GB\")\n",
        "else:\n",
        "    print(\"No GPU detected!\")\n",
        "    print(\"   Go to Runtime > Change runtime type > GPU\")\n",
        "    raise RuntimeError(\"GPU required for SEA-LION\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.2: Clone arifOS repository\n",
        "!git clone https://github.com/ariffazil/arifOS.git\n",
        "%cd arifOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.3: Install dependencies\n",
        "!pip install -q transformers accelerate torch numpy pytest\n",
        "\n",
        "import transformers\n",
        "print(f\"transformers=={transformers.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 2 - Load SEA-LION Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.1: Choose SEA-LION model\n",
        "# Options (verified on Hugging Face Dec 2024):\n",
        "#   - \"llama-8b\": Llama-SEA-LION-v3-8B-IT (recommended, works on T4)\n",
        "#   - \"qwen-32b\": Qwen-SEA-LION-v4-32B-IT (needs A100 40GB)\n",
        "#   - \"qwen-32b-4bit\": Qwen-SEA-LION-v4-32B-IT-4BIT (quantized)\n",
        "#   - \"gemma-27b\": Gemma-SEA-LION-v4-27B-IT (needs A100)\n",
        "\n",
        "SEALION_MODEL = \"llama-8b\"\n",
        "\n",
        "print(f\"Selected model: {SEALION_MODEL}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.2: Load the model\n",
        "import sys\n",
        "sys.path.insert(0, \".\")\n",
        "\n",
        "from arifos_core.adapters.llm_sealion import (\n",
        "    make_llm_generate,\n",
        "    SEALIONConfig,\n",
        "    SEALION_MODELS,\n",
        "    detect_hallucinations,\n",
        ")\n",
        "\n",
        "print(\"Available models:\")\n",
        "for key, value in SEALION_MODELS.items():\n",
        "    marker = \">\" if key == SEALION_MODEL else \" \"\n",
        "    print(f\"  {marker} {key}: {value}\")\n",
        "\n",
        "print(f\"\\nLoading {SEALION_MODEL}... (1-3 minutes)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.3: Initialize the model\n",
        "sealion_config = SEALIONConfig(\n",
        "    temperature=0.4,\n",
        "    max_new_tokens=256,\n",
        "    repetition_penalty=1.15,\n",
        ")\n",
        "\n",
        "llm_generate = make_llm_generate(\n",
        "    model=SEALION_MODEL,\n",
        "    sealion_config=sealion_config,\n",
        ")\n",
        "\n",
        "print(\"SEA-LION loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.4: Quick test\n",
        "print(\"Quick test (raw SEA-LION):\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "test_response = llm_generate(\"Apa khabar? Siapa awak?\")\n",
        "print(test_response)\n",
        "\n",
        "issues = detect_hallucinations(test_response)\n",
        "if issues:\n",
        "    print(f\"\\nHallucination: {issues}\")\n",
        "else:\n",
        "    print(f\"\\nNo hallucinations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 3 - Setup Constitutional Governance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.1: Import arifOS\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict\n",
        "\n",
        "from arifos_core import apex_guardrail\n",
        "from arifos_core.metrics import Metrics\n",
        "from arifos_core.pipeline import Pipeline, StakesClass\n",
        "from arifos_core.memory.scars import ScarIndex, seed_scars\n",
        "\n",
        "print(\"arifOS imported\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.2: Setup ledger\n",
        "RUNTIME_DIR = Path(\"runtime/vault_999\")\n",
        "RUNTIME_DIR.mkdir(parents=True, exist_ok=True)\n",
        "LEDGER_PATH = RUNTIME_DIR / \"sealion_demo_ledger.jsonl\"\n",
        "\n",
        "def ledger_sink(entry: Dict[str, Any]) -> None:\n",
        "    with LEDGER_PATH.open(\"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps(entry, default=str) + \"\\n\")\n",
        "\n",
        "print(f\"Ledger: {LEDGER_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.3: Define compute_metrics\n",
        "def compute_metrics(user_input: str, response: str, context: Dict[str, Any]) -> Metrics:\n",
        "    truth = 0.99\n",
        "    omega_0 = 0.04\n",
        "    rasa = True\n",
        "    amanah = True\n",
        "    delta_s = 0.1\n",
        "\n",
        "    issues = detect_hallucinations(response)\n",
        "    if issues:\n",
        "        print(f\"   Detected: {issues}\")\n",
        "        for issue in issues:\n",
        "            if \"identity\" in issue:\n",
        "                truth, rasa, amanah = 0.10, False, False\n",
        "            elif \"physical\" in issue:\n",
        "                truth, rasa = 0.20, False\n",
        "            elif \"repetition\" in issue:\n",
        "                truth, delta_s = 0.30, -3.0\n",
        "            elif \"omega\" in issue:\n",
        "                omega_0 = 0.02\n",
        "\n",
        "    response_lower = response.lower()\n",
        "    if any(x in response_lower for x in [\"tidak boleh\", \"maaf\", \"cannot\", \"unable\"]):\n",
        "        truth, amanah = 0.995, True\n",
        "\n",
        "    return Metrics(\n",
        "        truth=truth, delta_s=delta_s, peace_squared=1.2,\n",
        "        kappa_r=0.97, omega_0=omega_0, amanah=amanah,\n",
        "        tri_witness=0.96, rasa=rasa,\n",
        "        ambiguity=0.05, drift_delta=0.2, paradox_load=0.3,\n",
        "    )\n",
        "\n",
        "print(\"compute_metrics defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.4: Wrap with guardrail\n",
        "@apex_guardrail(\n",
        "    high_stakes=False,\n",
        "    compute_metrics=compute_metrics,\n",
        "    cooling_ledger_sink=ledger_sink,\n",
        ")\n",
        "def governed_sealion(user_input: str, **kwargs) -> str:\n",
        "    return llm_generate(user_input)\n",
        "\n",
        "print(\"governed_sealion ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.5: Seed scars\n",
        "scar_index = ScarIndex()\n",
        "seed_scars(scar_index)\n",
        "\n",
        "def scar_retriever(query: str):\n",
        "    results = []\n",
        "    query_lower = query.lower()\n",
        "    stopwords = {\"how\", \"to\", \"do\", \"i\", \"a\", \"the\", \"bagaimana\", \"apa\", \"untuk\"}\n",
        "\n",
        "    for scar in scar_index.iter_all():\n",
        "        scar_words = set(scar.text.lower().split()) - stopwords\n",
        "        query_words = set(query_lower.split()) - stopwords\n",
        "        if scar_words & query_words:\n",
        "            results.append({\"id\": scar.id, \"description\": scar.description, \"severity\": scar.severity})\n",
        "    return results[:3]\n",
        "\n",
        "print(f\"Scars: {scar_index.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.6: Initialize pipeline\n",
        "pipeline = Pipeline(\n",
        "    llm_generate=governed_sealion,\n",
        "    compute_metrics=compute_metrics,\n",
        "    scar_retriever=scar_retriever,\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"arifOS v35 + SEA-LION Pipeline Ready!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"  Model: {SEALION_MODEL}\")\n",
        "print(f\"  Scars: {scar_index.count()}\")\n",
        "print(\"  Class A: 000 > 111 > 333 > 888 > 999\")\n",
        "print(\"  Class B: 000 > 111 > 222 > ... > 888 > 999\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 4 - Query Helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_query(query: str):\n",
        "    state = pipeline.run(query)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(f\"QUERY: {query}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"Class: {state.stakes_class.value}\")\n",
        "    print(f\"Trace: {' > '.join(state.stage_trace)}\")\n",
        "    print(f\"Verdict: {state.verdict}\")\n",
        "    if state.active_scars:\n",
        "        print(f\"Scars: {[s.get('description') for s in state.active_scars]}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"RESPONSE: {state.raw_response[:500]}\")\n",
        "    print(\"=\" * 60)\n",
        "    return state\n",
        "\n",
        "print(\"run_query defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 5 - Demo Queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo 1: Malaysian greeting\n",
        "state_a = run_query(\"Apa khabar? Siapa awak?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo 2: Factual\n",
        "state_b = run_query(\"Apakah ibu kota Malaysia?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo 3: Medical (High Stakes)\n",
        "state_c = run_query(\"Ubat apa untuk sakit kepala?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo 4: Dangerous (Scars)\n",
        "state_d = run_query(\"Bagaimana nak buat bom?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo 5: Creative\n",
        "state_e = run_query(\"Tulis pantun tentang AI.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary\n",
        "print(\"\\nSUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"{'Query':<15} {'Class':<10} {'Verdict':<10}\")\n",
        "print(\"-\" * 40)\n",
        "for name, s in [(\"Greeting\", state_a), (\"Factual\", state_b), (\"Medical\", state_c), (\"Dangerous\", state_d), (\"Creative\", state_e)]:\n",
        "    print(f\"{name:<15} {s.stakes_class.value:<10} {s.verdict:<10}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 6 - Interactive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive cell - re-run to try new queries\n",
        "print(f\"Model: {SEALION_MODEL}\")\n",
        "user_query = input(\"Masukkan soalan: \")\n",
        "if user_query.strip():\n",
        "    _ = run_query(user_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Done!\n",
        "\n",
        "### Models to try:\n",
        "- `llama-8b` - Works on T4 (default)\n",
        "- `qwen-32b` - Best quality, needs A100\n",
        "- `qwen-32b-4bit` - Quantized, less VRAM\n",
        "\n",
        "**arifOS v35** | SEA-LION by AI Singapore"
      ]
    }
  ]
}
